import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns 
df = pd.read_csv('D:\\Users\\ASUS\\Documents\\car data.csv')
df=pd.read_csv('D:\\Users\\ASUS\\Documents\\CAR DETAILS FROM CAR DEKHO.csv')
df 
car_data = pd.read_csv('D:\\Users\\ASUS\\Documents\\car data.csv')
car_dekho =pd.read_csv('D:\\Users\\ASUS\\Documents\\CAR DETAILS FROM CAR DEKHO.csv')
print("Primeras filas de 'car data':")
print(car_data)
print(car_dekho)
print("\nNombres de las columnas en 'car data':")
print(car_data.columns.tolist())
print("\nNombres de las columnas en 'CAR DETAILS FROM CAR DEKHO.csv:")
print(car_dekho.columns.tolist())
car_data.columns = car_data.columns.str.strip()
car_dekho.columns = car_dekho.columns.str.strip()
print("\nNombres de las columnas después de limpiar en 'car data':")
print(car_data.columns.tolist())
print("\nNombres de las columnas después de limpiar en 'CAR DETAILS FROM CAR DEKHO':")
print(car_dekho.columns.tolist())
if 'Selling_Price' in car_data.columns: 
    print("\n'Selling_Price' existe en 'car data'")
else: 
    print("\n'Selling_Price' NO existe en 'car data'")
df= pd.read_csv('D:\\Users\\ASUS\\Documents\\car data.csv')
plt.figure(figsize=(8, 5))
sns.boxplot(x=df['Year'])
plt.show()
plt.figure(figsize=(7, 5))
sns.boxplot(x=df['Kms_Driven'])
plt.show()
plt.figure(figsize=(6, 6))
sns.scatterplot(x='Year', y='Kms_Driven', data=df)
plt.title('Gráfico de Dispersión entre Year y Kms Drive')
plt.xlabel('variable independiente Year')
plt.ylabel(' variable dependiente Kms Drive')
plt.show()
max_Owner= 10
min_Owner= 2
total_Owner = ((df['Owner'] < min_Owner) | (df['Owner'] > max_Owner)).sum() 
max_Present_Price= 20
min_Present_Price= 8 
total_Present_Price = ((df['Present_Price'] < min_Present_Price) |  (df ['Present_Price'] >max_Present_Price)).sum()

print("Total registros dataset:{}".format(len(df)))
print("El total de datos atípicos para la cantidad de Kms_Driven es {}".format(total_Owner))
print("El total de datos atípicos para el Present_Price es {}".format(total_Present_Price))
print ("")
df.info()
print(df. isnull().sum())
print(type(df))
df.mean(numeric_only=True)
from sklearn.preprocessing import LabelEncoder
df_numerica= df.copy()
for column in df_numerica.columns:
    if df_numerica[column].dtype== 'object':
            label_encoder= LabelEncoder()
    df_numerica[column]= label_encoder.fit_transform(df_numerica[column])
from sklearn.model_selection import train_test_split
X= df_numerica.drop('Present_Price', axis=1)
Y= df_numerica['Car_Name']
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=0)
X_train.head(2)
X_test.head(2)
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression

df_numerica = pd.read_csv('D:\\Users\\ASUS\\Documents\\car data.csv')

# Preparar X y Y
X = df_numerica.drop('Car_Name', axis=1)
Y = df_numerica['Present_Price']
X = pd.get_dummies(X, drop_first=True)

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=0)

modelo_regresion = LinearRegression()
modelo_regresion.fit(X_train, Y_train)
from sklearn.metrics import mean_squared_error, r2_score
Prediccion = modelo_regresion.predict(X_test)

from sklearn.metrics import mean_squared_error, r2_score

print('Coeficientes:', modelo_regresion.coef_)
print('Error cuadrático medio:', mean_squared_error(Y_test, Prediccion))
print('Coeficiente de determinación (R²):', r2_score(Y_test, Prediccion))
import matplotlib.pyplot as plt
import seaborn as sns
residuos = y_train - y_test  
plt.figure(figsize=(5, 5))
sns.scatterplot(x= y_train, y=residuos, color='blue', alpha=0.5)
plt.axhline(0, color='red', linestyle='--')
plt.title ('Grafico de residuos')
plt.xlabel('Predicciones')
plt.ylabel ('Residuos')
plt.grid()
plt.show()

plt.scatter (Y_test, Prediccion)
plt.xlabel("Present_Price")
plt.ylabel("Predicciones")
plt.title("Rendimiento del modelo de regresión lineal")
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns
errors = y_test-y_pred
plt.figure(figsize=(8,6))
sns.histplot(errors, bins=30, kde=True, color='blue')
plt.title('Histograma de Errores')
plt.xlabel('Error')
plt.ylabel('Frecuencia')
plt.show()

import pandas as pd

car_data = pd.read_csv('D:\\Users\\ASUS\\Documents\\car data.csv')  # <-- Aquí cargas el dataset

X = car_data[['Present_Price', 'Year', 'Kms_Driven']]
Y = car_data['Selling_Price']

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)
print("Dimensiones del conjunto de entrenamiento:")
print("X_train:", X_train.shape)
print("y_train:", y_train.shape)
print("\nDimensiones del conjunto de prueba:")
print( "X_test:", X_test.shape)
print( "y_test:", y_test.shape)

import pandas as pd
from sklearn.ensemble import RandomForestRegressor

model = RandomForestRegressor()
model.fit(X_train, y_train)

importance = model.feature_importances_

feature_importances = pd.DataFrame({
    'Feature': X.columns,
    'Importance': importance})

feature_importances = feature_importances.sort_values(by='Importance', ascending=False)
print(feature_importances)
plt.figure(figsize=(5,5))
sns.barplot(x='Importance', y='Feature', data= feature_importances)
plt.title('Importancia de las caracteristicas')
plt.xlabel('Importancia')
plt.ylabel('Caracteristicas')
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns 

sns.set(style="whitegrid") 

df = pd.read_csv('D:\\Users\\ASUS\\Downloads\\archive (2)\\heart_cleveland_upload.csv')

print("Primeras filas del DataFrame:")
print(df.head())

print("\nInformación del DataFrame:")
print(df.info)

print("\nValores nulos en el DataFrame:")
print(df.isnull().sum)

print("\nResumen estadistico:")
print(df.describe())

df[['age', 'chol']].hist(bins=15, figsize=(10,5))
plt.suptitle('Histograma de las variables de interés', fontsize=16)
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('D:\\Users\\ASUS\\Downloads\\archive (2)\\heart_cleveland_upload.csv')

plt.figure(figsize=(8, 6))
sns.scatterplot(data=df, x='age', y='chol')
plt.title('Relación entre Edad y Colesterol')
plt.xlabel('variable independiente edad')
plt.ylabel(' variable dependiente colesterol')
plt.show()

duplicates= df.duplicated().sum()
print(f"Registros duplicados: {duplicates}")
if duplicates> 0:
   data=data.drop_duplicates()

categorical_columns= ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal']
for col in categorical_columns: 
    df [col]= df [col].astype('category')

from sklearn.preprocessing import StandardScaler

numeric_columns= ['age', 'trestbps', 'chol', 'thalach', 'oldspeak']
scaler=StandardScaler()

print(df.columns)
numeric_columns = [col for col in numeric_columns if col in df.columns]
df[numeric_columns] = scaler.fit_transform(df[numeric_columns])

print("\nDatos después del procesamiento:")
print(df.describe())

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split 
import matplotlib.pyplot as plt

df.dropna(inplace=True)
X= df.drop('condition', axis=1)
y= df ['condition']

import pandas as pd
from sklearn.model_selection import train_test_split

df = pd.read_csv('D:\\Users\\ASUS\\Downloads\\archive (2)\\heart_cleveland_upload.csv')

X = df.drop('condition', axis=1)  
y = df['condition']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print (X_train)
print (y_test)

model=RandomForestClassifier(random_state=42)
model.fit(X_train, y_train)
importances= model.feature_importances_
feature_importances=pd.DataFrame({'Feature': X.columns,'Importance': importances})

plt.figure(figsize=(10,6))
sns.barplot(x='Importance', y='Feature', data= feature_importances)
plt.title ('Importancia de las Características')
plt.xlabel ('Importancia')
plt.ylabel ('Características')
plt.show()

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report,accuracy_score 
from sklearn.linear_model import LogisticRegression

model = LogisticRegression(solver='liblinear')
model.fit(X_train, y_train)

from sklearn.metrics import accuracy_score, classification_report

accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)

print("Precisión del modelo:", accuracy)

from sklearn.metrics import classification_report, accuracy_score
y_pred= model.predict (X_test)
accuracy= accuracy_score (y_test, y_pred)
print ("Precisión del modelo:", accuracy)

report= classification_report (y_test, y_pred)
print("Informe de clasificación:\n", report)

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import seaborn as sns

y_pred = model.predict(X_test) 
cm = confusion_matrix(y_test, y_pred)

plt.figure(figsize=(8, 6))
ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Sin Enfermedad', 'Con Enfermedad']).plot(cmap='Blues')
plt.title('Matriz de Confusión')
plt.show()

import numpy as np
metrics = ['Precisión', 'Recall', 'F1-score']
values = [0.77, 0.72, 0.74] # Promedio de las métricas para la clase
0
values_1 = [0.70, 0.75, 0.72] # Promedio de las métricas para la
bar_width = 0.35
clase1_bar_width = 0.35
x = np.arange(len(metrics)) # El eje x
plt.figure(figsize=(10, 5))
plt.bar(x - bar_width/2, values, bar_width, label='Clase 0 (Sin Enfermedad)', color='blue')
plt.bar(x + bar_width/2, values_1, bar_width, label='Clase 1 (Con Enfermedad)', color='orange')
plt.xticks(x, metrics)
plt.ylim(0, 1)
plt.ylabel('Valor')
plt.title('Métricas de Evaluación por Clase')
plt.legend()
plt.show()

from sklearn.metrics import roc_curve, roc_auc_score
y_pred_proba = model.predict_proba(X_test)[:, 1]
fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)
roc_auc = roc_auc_score(y_test, y_pred_proba)
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, label='Curva ROC (AUC = {:.2f})'.format(roc_auc))
plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
plt.xlabel('Tasa de Falsos Positivos')
plt.ylabel('Tasa de Verdaderos Positivos')
plt.title('Curva ROC')
plt.legend()
plt.show()

# 4. Gráfico de distribución de probabilidades
plt.figure(figsize=(10, 6))
sns.histplot(y_pred_proba, bins=10, kde=True)
plt.axvline(0.5, color='red', linestyle='--') # Umbral de decisión
plt.xlabel('Probabilidad Predicha')
plt.ylabel('Frecuencia')
plt.title('Distribución de Probabilidades Predichas')
plt.show()

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('D:/Users/ASUS/Downloads/archive (3)/winequality-red.csv')

import pandas as pd

data = pd.read_csv('D:/Users/ASUS/Downloads/archive (3)/winequality-red.csv')  # Ajusta la ruta según tu caso

print("Primeras filas del dataset:")
print(data.head())

print("\nInformación general del dataset:")
print(data.info())

print("\nResumen estadistico de las variables numericas:")
print(data.describe())

import matplotlib.pyplot as plt
print("\nVisualización de histogramas:")
data.hist(figsize=(10,10), bins=30, edgecolor= 'black')
plt.suptitle("Distribución de las variables numericas", fontsize=16)
plt.show()

print("\nGráfico de dispersión entre variables 'alcohol', y 'quality':")
data.plot(kind='scatter', x='alcohol', y='quality', alpha=0.5)
plt.title("alcohol vs quality", fontsize=16)
plt.show()

import seaborn as sns
print("\nMatriz de corelación:")
correlation_matrix=data.corr()
plt.figure(figsize=(10,8))
sns.heatmap(correlation_matrix, annot= True, cmap='coolwarm', fmt=".2f")
plt.title("Matriz de correlación", fontsize=16)
plt.show()

from sklearn.preprocessing import StandardScaler
import numpy as np
Q1 = data.quantile(0.25)
Q3 = data.quantile(0.75)
IQR = Q3 - Q1
data_clean = data[~((data < (Q1 - 1.5 * IQR)) | (data > (Q3 + 1.5 *
IQR))).any(axis=1)]
print(f"Número de filas después de eliminar outliers:{data_clean.shape[0]}")
scaler = StandardScaler()
features = data_clean.drop('quality', axis=1)
data_scaled = pd.DataFrame(scaler.fit_transform(features),
columns=features.columns)
data_scaled['quality'] = data_clean['quality'].values
data_scaled['quality_category'] = pd.cut(data_scaled['quality'], bins=[2,
5, 6, 8], labels=['Baja', 'Media', 'Alta'])
print(data_scaled.head())

import numpy as np 
import matplotlib.pyplot as plt 
import seaborn as sns 
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import mutual_info_score 

X=data.drop(columns=['quality'])
y=data['quality']

rf= RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X,y)

importances=rf.feature_importances_
indices= np.argsort(importances)[::-1]

plt.figure(figsize=(10,6))
plt.title("Importancia de las caracteristicas")
plt.bar(range(X.shape[1]), X.columns[indices], align='center')
plt.xticks(range(X.shape[1]), X.columns[indices], rotation= 90)
plt.xlim([-1, X.shape[1]])
plt.ylabel("Importancia")
plt.show()

from sklearn.model_selection import train_test_split
X = data.drop(columns=['quality']) 
y = data['quality']
X_train, X_test, y_train, y_test = train_test_split(X, y,
test_size=0.2, random_state=42)
print(f'Dimensiones del conjunto de entrenamiento: {X_train.shape},{y_train.shape}')
print(f'Dimensiones del conjunto de prueba: {X_test.shape},{y_test.shape}')

from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import cross_val_score
dt = DecisionTreeClassifier(random_state=42)
scores = cross_val_score(dt, X_train, y_train, cv=5)
print("Precisión media del modelo:", scores.mean())

from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report, confusion_matrix
X = data.drop(columns=['quality'])
y = data['quality']
X_train, X_test, y_train, y_test = train_test_split(X, y,
test_size=0.2, random_state=42)
model = DecisionTreeClassifier(random_state=42)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
report = classification_report(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
print("Reporte de Clasificación:\n", report)
print("Matriz de Confusión:\n", conf_matrix)

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report
import numpy as np
import pandas as pd
plt.figure(figsize=(10, 7))
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
xticklabels=np.unique(y), yticklabels=np.unique(y))
plt.xlabel('Predicción')
plt.ylabel('Verdadero')
plt.title('Matriz de Confusión')
plt.show()

report = classification_report(y_test, y_pred, output_dict=True)
metrics = ['precision', 'recall', 'f1-score']
classes = report.keys()
metrics_data = {metric: [report[cls][metric] for cls in classes if cls
not in ['accuracy', 'macro avg', 'weighted avg']] for metric in
metrics}
metrics_df = pd.DataFrame(metrics_data)
metrics_df.index = [cls for cls in classes if cls not in ['accuracy',
'macro avg', 'weighted avg']]
metrics_df.plot(kind='bar', figsize=(10, 6))
plt.title('Comparación de Métricas por Clase')
plt.ylabel('Valor')
plt.xlabel('Clases')
plt.xticks(rotation=0)
plt.legend(title='Métricas')
plt.show()

plt.figure(figsize=(10, 6))
sns.countplot(x=y_pred)
plt.title('Distribución de las Predicciones')
plt.xlabel('Clases Predichas')
plt.ylabel('Número de Instancias')
plt.show()

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

y_pred = modelo_regresion.predict(X_test)

mse = mean_squared_error(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("Error cuadrático medio:", mse)
print("Error absoluto medio:", mae)
print("Coeficiente de determinación (R²):", r2)
